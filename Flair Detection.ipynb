{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def get_data(train_file, test_file = None):\n    if test_file == None:\n        frame = pd.read_csv(train_file)\n        print(frame.head(5))\n        data = frame.values\n        np.random.shuffle(data)\n        return data\n    else:\n        train_frame = pd.read_csv(train_file)\n        test_frame = pd.read_csv(test_file, error_bad_lines=False, quoting = 2)\n\n        train_data = train_frame.values\n        test_data = test_frame.values\n        np.random.shuffle(train_data)\n        np.random.shuffle(test_data)\n\n        return train_data, test_data\n\ndef get_training_testing_sets(train_file, test_file = None):\n    if test_file == None:\n        data = get_data(train_file)\n        train_data, test_data = train_test_split(data)\n    else:\n\n        train_data, test_data = get_data(train_file, test_file)\n\n    X_train = train_data[:, 1:]\n    Y_train = train_data[:, :1]\n    X_test = test_data[:, 1:]\n    Y_test = test_data[:, :1]\n\n    print(X_train.shape, X_test.shape)\n    \n    return X_train, Y_train, X_test, Y_test\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, test_data = get_data('Subreddit_India_Train.csv', 'Subreddit_India_Test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport string\n\nimport nltk\nfrom nltk.tokenize.casual import _replace_html_entities\n# refer: http://www.nltk.org/_modules/nltk/tokenize/casual.html#TweetTokenizer\n# from utils.contractions import expand_contractions\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nstopwords = spacy.lang.en.stop_words.STOP_WORDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleanup(text):\n    text = text.lower()\n    text = _replace_html_entities(text) # fix HTML character entities\n#     text = expand_contractions(text) # expand contractions\n    text = re.sub(r'@\\S+', '', text) # remove mentions\n    text = re.sub(r'(www\\.\\S+)|(https?\\://\\S+)', '', text) # remove urls\n    text = re.sub(r'#(\\S+)', r'\\1', text) # replaces #hashtag with hashtag\n    text = re.sub(r'\\brt\\b', '', text) # remove RT\n    text = re.sub(r'\\'s', '', text) # remove possession apostrophe\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    text = re.sub(r'(\\.{2,})|(\\s+)', ' ', text) # replace 2+ dots/spaces with a single space\n    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) # remove non-alphanumeric chars (punctuations also removed)\n    text = re.sub(r'(.)\\1+', r'\\1\\1', text) # replace repeated char seq of length >=2 with seq of length 2\n    return text\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nimport string\nfrom nltk.stem import WordNetLemmatizer\n\n\npunctuations = string.punctuation\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess(sentence):\n    if str(sentence) == 'nan':\n        return ''\n    sentence = cleanup(sentence)\n    sentence = ''.join(j for j in sentence if j not in punctuations)\n    sentence = ' '.join(lemmatizer.lemmatize(j.lower()) for j in sentence.split())\n    return sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for row in range(len(train_data)):\n    train_data[row][3] = preprocess(train_data[row][3]) + ' ' + preprocess(train_data[row][4])\n    train_data[row][4] = ''\nfor row in range(len(test_data)):\n    test_data[row][3] = preprocess(test_data[row][3]) + ' ' + preprocess(test_data[row][4])\n    test_data[row][4] = ''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nX_train = train_data[:, 3]\nX_test = test_data[:, 3]\n\nencoder = LabelEncoder()\nY_train = encoder.fit_transform(data[:, 1:2])\nY_test = encoder.fit_transform(data[:, 1:2])\n\nY_train = np.array(Y_train)\nY_test = np.array(Y_test)\n\n\nprint('X_train', X_train.shape)\nprint('X_test', X_test.shape)\nprint('Y_train', Y_train.shape)\nprint('Y_test', Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport warnings\n\nprint(encoder.classes_)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    for clf, name in [(OneVsRestClassifier(BernoulliNB()),'BernoulliNB'), (RandomForestClassifier(max_depth = 30), 'RandomForestClassifier', ), (OneVsRestClassifier(LogisticRegression(C = 0.2)), 'LogisticRegression'), (OneVsRestClassifier(LinearSVC(C = 0.1)), 'SVC') ]:\n#     for clf, name in [(OneVsRestClassifier(LinearSVC(C = 0.06)), 'SVC') ]:\n        print(name)\n    #     Y_train.reshape(Y_train.shape[0],)\n    #     Y_test.reshape(Y_test.shape[0])\n        clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', clf)])\n        clf.fit(X_train, Y_train)\n        \n        predictions = clf.predict(X_train)\n        accuracy = accuracy_score(Y_train, predictions)\n        print('training accuracy :',accuracy)\n\n        predictions = clf.predict(X_test)\n#         print(predictions[1], Y_test[1])\n        accuracy = accuracy_score(Y_test, predictions)\n        print('testing accuracy :',accuracy)\n\n        print(classification_report(Y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\npickle.dump(clf, open('model', 'wb'))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}